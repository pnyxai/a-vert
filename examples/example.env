# ===========================
# GENERAL CONFIGURATION
# ===========================

# Set to 1 to use only cached models (no downloads), 0 to allow downloads
HF_HUB_OFFLINE=1

# Hugging Face API Token (required for downloading models)
HF_TOKEN=hf_your_token_here

# Path to your models directory (will be mounted into containers)
MODELS_PATH=/path/to/models

# Number of GPUs to use for tensor parallelism
NUM_GPUS=1

# ===========================
# A-VERT MODEL CONFIGURATION
# ===========================

# GPU device IDs to use for A-VERT (e.g., '0' or '0,1' for multiple GPUs)
AVERT_GPU_IDS=0

# Task type for the A-VERT model (e.g., "embed", "score", "classify")
TASK=classify

# Model name/path for A-VERT (Hugging Face model identifier or local path)
AVERT_MODEL_NAME=org/embedding-model

# Name to serve the A-VERT model under (used in API requests)
AVERT_SERVED_MODEL_NAME=avert-model

# GPU memory utilization for A-VERT (0.0 to 1.0)
AVERT_GPU_MEMORY_UTILIZATION=0.15

# Maximum model length (context window) for A-VERT
AVERT_MAX_MODEL_LEN=512

# Maximum number of sequences to process in parallel for A-VERT
AVERT_MAX_NUM_SEQS=8

VLLM_LOGGING_LEVEL=INFO

# ===========================
# LLM CONFIGURATION
# ===========================

# GPU device IDs to use for LLM (e.g., '0' or '0,1' for multiple GPUs)
LLM_GPU_IDS=0

# Model name/path for the main LLM (Hugging Face model identifier or local path)
LLM_MODEL_NAME=org/llm-model

# Name to serve the LLM under (used in API requests)
LLM_SERVED_MODEL_NAME=llm-model

# GPU memory utilization for LLM (0.0 to 1.0)
LLM_GPU_MEMORY_UTILIZATION=0.75

# Maximum model length (context window) for LLM
LLM_MAX_MODEL_LEN=16384

# Maximum number of sequences to process in parallel for LLM
LLM_MAX_NUM_SEQS=4